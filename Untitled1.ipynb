{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "4512caea-7b73-4ab5-964a-44ba27353606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from triton import heuristics, jit\n",
    "from triton import language as tl\n",
    "from triton import next_power_of_2\n",
    "\n",
    "import triton\n",
    "\n",
    "\n",
    "def num_warps(N):\n",
    "    if N < 2048:\n",
    "        return 4\n",
    "    elif N < 8192:\n",
    "        return 8\n",
    "    return 16\n",
    "\n",
    "\n",
    "\n",
    "def num_warps(N):\n",
    "    if N < 2048:\n",
    "        return 4\n",
    "    elif N < 8192:\n",
    "        return 8\n",
    "    return 16\n",
    "\n",
    "def num_warps(N):\n",
    "    if N < 2048:\n",
    "        return 4\n",
    "    elif N < 8192:\n",
    "        return 8\n",
    "    return 16\n",
    "\n",
    "\n",
    "@heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n",
    "@heuristics({'BLOCK': lambda nargs: next_power_of_2(nargs['N'])})\n",
    "@jit\n",
    "def _forward(LOGITS, PROBS, IDX, LOSS, N, BLOCK: tl.constexpr):\n",
    "    row = tl.program_id(0)\n",
    "    cols = tl.arange(0, BLOCK)\n",
    "    idx = tl.load(IDX + row)\n",
    "    ignore_index = -100\n",
    "    # pointers to logit and probs\n",
    "    LOGITS = LOGITS + row * N + cols\n",
    "    WRIT_PROBS = PROBS + row * N + cols\n",
    "    READ_PROBS = PROBS + row * N + idx\n",
    "    # write-back negative log-probs\n",
    "    logits = tl.load(LOGITS, mask=cols < N, other=-float('inf'))\n",
    "    logits = logits.to(tl.float32)\n",
    "    logits = logits - tl.max(logits, 0)\n",
    "    probs_left = tl.log(tl.sum(tl.exp(logits), 0))\n",
    "    probs = probs_left - logits\n",
    "\n",
    "    probs_loss = probs_left - tl.sum(tl.where(cols == idx, logits, 0.0))\n",
    "    probs_loss = tl.where(idx == ignore_index, 0.0, probs_loss)\n",
    "    # tl.store(WRIT_PROBS, probs, mask=cols < N)\n",
    "\n",
    "    # There is a bug in the compiler, which fails to insert a barrier here.\n",
    "    # We add it explicitly for now. Will be fixed soon.\n",
    "    # tl.debug_barrier()\n",
    "    # write-back loss\n",
    "    # probs_loss = tl.load(READ_PROBS)\n",
    "    # probs_loss = tl.where(idx == ignore_index, 0.0, probs_loss)\n",
    "    tl.store(LOSS + row, probs_loss)\n",
    "\n",
    "    tl.debug_barrier()\n",
    "    probs = -probs\n",
    "    probs = tl.exp(probs.to(tl.float32))\n",
    "    delta = cols == idx\n",
    "    din = (probs - delta)\n",
    "    din = tl.where(idx == ignore_index, 0.0, din)\n",
    "    tl.store(WRIT_PROBS, din, mask=cols < N)\n",
    "\n",
    "class _cross_entropy(torch.autograd.Function):\n",
    "\n",
    "    @classmethod\n",
    "    def forward(cls, ctx, hidden_states, indices, weights):\n",
    "        logits = torch.matmul(hidden_states, weights.T)\n",
    "        logits = logits.float()\n",
    "        # make sure we can use triton\n",
    "        assert (indices.dtype == torch.int64), \"Indices are expected to be of type long.\"\n",
    "        # make kernel\n",
    "        device, dtype = logits.device, logits.dtype\n",
    "        n_cols = logits.shape[-1]\n",
    "        # run the kernel\n",
    "        result = torch.empty_like(indices, dtype=dtype, device=device)\n",
    "        neg_logprobs = torch.empty_like(logits, dtype=dtype, device=device)\n",
    "        grid = lambda opt: (logits.numel() // n_cols, )\n",
    "        _forward[grid](logits, neg_logprobs, indices, result, n_cols)\n",
    "        # save for backward\n",
    "        neg_logprobs = neg_logprobs.to(torch.bfloat16)\n",
    "        grad_input = neg_logprobs @ weights\n",
    "\n",
    "        ignore_index = -100\n",
    "        mask = (indices != ignore_index)\n",
    "\n",
    "        if hasattr(weights, 'grad') and weights.grad != None:\n",
    "            torch.addmm(\n",
    "                    weights.grad,\n",
    "                    neg_logprobs.T,\n",
    "                    hidden_states,\n",
    "                    out=weights.grad,\n",
    "                )\n",
    "        else:\n",
    "            weights.grad = neg_logprobs.T @ hidden_states\n",
    "\n",
    "        \n",
    "        if hasattr(weights, 'mul') and weights.mul != None:\n",
    "            weights.mul += torch.sum(indices != ignore_index) \n",
    "        else:\n",
    "            weights.mul = torch.sum(indices != ignore_index) \n",
    "            \n",
    "        weights.grad_mul = False\n",
    "        \n",
    "        neg_logprobs = None\n",
    "\n",
    "        ctx.save_for_backward(grad_input, weights)\n",
    "        return result[mask].mean()\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, ctx, dneg_logprobs):\n",
    "        \"\"\"We know d(-log(p[i])/dlogit[k] = -id_mat[i,k] + p[k]\n",
    "        so we initialize the gradient as neg_logprobs, so we can just exponentiate\n",
    "        to get p[k], which is most of what we need...  neg_logprobs will be\n",
    "        modified in place to become the gradient we want\n",
    "        \"\"\"\n",
    "        # load saved tensors\n",
    "        neg_logprobs, weights = ctx.saved_tensors\n",
    "        \n",
    "        dneg_logprobs = dneg_logprobs / weights.mul\n",
    "        if weights.grad_mul is False:\n",
    "            weights.grad *= dneg_logprobs\n",
    "            weights.grad_mul = True\n",
    "        neg_logprobs *= dneg_logprobs\n",
    "        \n",
    "        return neg_logprobs, None, weights.grad\n",
    "\n",
    "\n",
    "class FusedCrossEntropyLMhead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_weight = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if original_weight is None:\n",
    "            self.LM_head_weight = nn.Parameter(torch.empty(hidden_size, vocab_size))\n",
    "        else:\n",
    "            self.LM_head_weight = original_weight\n",
    "        self.cross_entropy = _cross_entropy.apply\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        ignore_index = -100\n",
    "        loss = self.cross_entropy(hidden_states, labels, self.LM_head_weight)\n",
    "        # mask = (labels != ignore_index)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "f66b3765-a698-4937-ba35-668f3e889f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 8\n",
    "N = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "51efea54-4788-4f8b-98d0-50665100bfd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.3438, 2.4062, 3.0312, 4.1250, 2.7656, 3.1094, 3.0625, 4.6562],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<_cross_entropyBackward>) tensor([3.3438, 2.4062, 3.0312, 4.1250, 2.7656, 3.1094, 3.0625, 4.6562],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>) tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from triton.ops import cross_entropy\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn(M, N, dtype=dtype, device=device, requires_grad=True)\n",
    "\n",
    "random_sequence = np.random.randint(low=0, high=N, size=(M,))\n",
    "idx = torch.tensor(random_sequence).cuda() \n",
    "\n",
    "tt_y = triton.ops.cross_entropy(x, idx)\n",
    "th_y = torch.nn.CrossEntropyLoss(reduction=\"none\")(x, idx)\n",
    "\n",
    "print(tt_y, th_y, torch.sum(tt_y - th_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "3cc0d6c3-755e-4472-a6dd-dd7b84c32950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5781, 2.7500, 1.8828, 2.4844, 2.7656, 1.4688, 2.9844, 2.0312],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<_cross_entropyBackward>) tensor([2.5781, 2.7500, 1.8828, 2.4844, 2.7656, 1.4688, 2.9844, 2.0312],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>) tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from triton.ops import cross_entropy\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn(M, N, dtype=dtype, device=device, requires_grad=True)\n",
    "random_sequence = np.random.randint(low=0, high=N, size=(M,))\n",
    "idx = torch.tensor(random_sequence).cuda() \n",
    "lm_head = nn.Linear(N, N, bias=False).cuda()\n",
    "lm_head.bfloat16()\n",
    "\n",
    "\n",
    "tt_y = triton.ops.cross_entropy(lm_head(x), idx)\n",
    "th_y = torch.nn.CrossEntropyLoss(reduction=\"none\")(lm_head(x), idx)\n",
    "print(tt_y, th_y, torch.sum(tt_y - th_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "185f9e24-8981-469a-a157-1fa09f4a4dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   4,    7,    4,    7,    0,    0, -100, -100], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'builtin_function_or_method' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[492], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m fuse \u001b[38;5;241m=\u001b[39m FusedCrossEntropyLMhead(lm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m     18\u001b[0m th_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)(lm_head(x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\u001b[38;5;241m.\u001b[39mfloat(), idx[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 19\u001b[0m tt_y \u001b[38;5;241m=\u001b[39m \u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m th_y\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m tt_y\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[488], line 150\u001b[0m, in \u001b[0;36mFusedCrossEntropyLMhead.forward\u001b[0;34m(self, hidden_states, labels)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, labels):\n\u001b[1;32m    149\u001b[0m     ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m--> 150\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLM_head_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# mask = (labels != ignore_index)\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[488], line 106\u001b[0m, in \u001b[0;36m_cross_entropy.forward\u001b[0;34m(cls, ctx, hidden_states, indices, weights)\u001b[0m\n\u001b[1;32m    102\u001b[0m     weights\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m neg_logprobs\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m hidden_states\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(weights, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmul\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     weights\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(indices \u001b[38;5;241m!=\u001b[39m ignore_index) \n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     weights\u001b[38;5;241m.\u001b[39mmul \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(indices \u001b[38;5;241m!=\u001b[39m ignore_index) \n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'builtin_function_or_method' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "from triton.ops import cross_entropy\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn(M, N, dtype=dtype, device=device, requires_grad=True)\n",
    "x_test = x.detach().clone()\n",
    "x_test.requires_grad=True\n",
    "random_sequence = np.random.randint(low=0, high=N, size=(M,))\n",
    "idx = torch.tensor(random_sequence).cuda() \n",
    "idx[-2:] = -100\n",
    "print(idx)\n",
    "lm_head = nn.Linear(N, N, bias=False).cuda()\n",
    "lm_head.bfloat16()\n",
    "\n",
    "fuse = FusedCrossEntropyLMhead(lm_head.weight.detach().clone())\n",
    "\n",
    "th_y = torch.nn.CrossEntropyLoss(reduction=\"mean\")(lm_head(x[:-1, :]).float(), idx[1:])\n",
    "tt_y = fuse(x_test[:-1, :], idx[1:])\n",
    "\n",
    "th_y.backward()\n",
    "tt_y.backward()\n",
    "\n",
    "print(lm_head.weight.grad, fuse.LM_head_weight.grad)\n",
    "print(x.grad , x_test.grad)\n",
    "print(torch.sum(lm_head.weight.grad - fuse.LM_head_weight.grad))\n",
    "print(torch.sum(x.grad - x_test.grad))\n",
    "\n",
    "print(tt_y, th_y, torch.sum(tt_y - th_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "34f2df46-f42b-4b36-8e51-06fb1028c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from triton import heuristics, jit\n",
    "from triton import language as tl\n",
    "from triton import next_power_of_2\n",
    "\n",
    "import triton\n",
    "\n",
    "\n",
    "def num_warps(N):\n",
    "    if N < 2048:\n",
    "        return 4\n",
    "    elif N < 8192:\n",
    "        return 8\n",
    "    return 16\n",
    "\n",
    "\n",
    "\n",
    "def num_warps(N):\n",
    "    if N < 2048:\n",
    "        return 4\n",
    "    elif N < 8192:\n",
    "        return 8\n",
    "    return 16\n",
    "\n",
    "def num_warps(N):\n",
    "    if N < 2048:\n",
    "        return 4\n",
    "    elif N < 8192:\n",
    "        return 8\n",
    "    return 16\n",
    "\n",
    "\n",
    "@heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n",
    "@heuristics({'BLOCK': lambda nargs: next_power_of_2(nargs['N'])})\n",
    "@jit\n",
    "def _forward(LOGITS, PROBS, IDX, LOSS, N, BLOCK: tl.constexpr):\n",
    "    row = tl.program_id(0)\n",
    "    cols = tl.arange(0, BLOCK)\n",
    "    idx = tl.load(IDX + row)\n",
    "    ignore_index = -100\n",
    "    # pointers to logit and probs\n",
    "    LOGITS = LOGITS + row * N + cols\n",
    "    WRIT_PROBS = PROBS + row * N + cols\n",
    "    READ_PROBS = PROBS + row * N + idx\n",
    "    # write-back negative log-probs\n",
    "    logits = tl.load(LOGITS, mask=cols < N, other=-float('inf'))\n",
    "    logits = logits.to(tl.float32)\n",
    "    logits = logits - tl.max(logits, 0)\n",
    "    probs_left = tl.log(tl.sum(tl.exp(logits), 0))\n",
    "    probs = probs_left - logits\n",
    "\n",
    "    probs_loss = probs_left - tl.sum(tl.where(cols == idx, logits, 0.0))\n",
    "    probs_loss = tl.where(idx == ignore_index, 0.0, probs_loss)\n",
    "    # tl.store(WRIT_PROBS, probs, mask=cols < N)\n",
    "\n",
    "    # There is a bug in the compiler, which fails to insert a barrier here.\n",
    "    # We add it explicitly for now. Will be fixed soon.\n",
    "    # tl.debug_barrier()\n",
    "    # write-back loss\n",
    "    # probs_loss = tl.load(READ_PROBS)\n",
    "    # probs_loss = tl.where(idx == ignore_index, 0.0, probs_loss)\n",
    "    tl.store(LOSS + row, probs_loss)\n",
    "\n",
    "    tl.debug_barrier()\n",
    "    probs = -probs\n",
    "    probs = tl.exp(probs.to(tl.float32))\n",
    "    delta = cols == idx\n",
    "    din = (probs - delta)\n",
    "    din = tl.where(idx == ignore_index, 0.0, din)\n",
    "    tl.store(WRIT_PROBS, din, mask=cols < N)\n",
    "\n",
    "class _cross_entropy(torch.autograd.Function):\n",
    "\n",
    "    @classmethod\n",
    "    def forward(cls, ctx, hidden_states, indices, weights):\n",
    "        logits = torch.matmul(hidden_states, weights.T)\n",
    "        logits = logits.float()\n",
    "        # make sure we can use triton\n",
    "        assert (indices.dtype == torch.int64), \"Indices are expected to be of type long.\"\n",
    "        # make kernel\n",
    "        device, dtype = logits.device, logits.dtype\n",
    "        n_cols = logits.shape[-1]\n",
    "        # run the kernel\n",
    "        result = torch.empty_like(indices, dtype=dtype, device=device)\n",
    "        neg_logprobs = torch.empty_like(logits, dtype=dtype, device=device)\n",
    "        grid = lambda opt: (logits.numel() // n_cols, )\n",
    "        _forward[grid](logits, neg_logprobs, indices, result, n_cols)\n",
    "        # save for backward\n",
    "        neg_logprobs = neg_logprobs.to(torch.bfloat16)\n",
    "        grad_input = neg_logprobs @ weights\n",
    "\n",
    "        ignore_index = -100\n",
    "        mask = (indices != ignore_index)\n",
    "\n",
    "        if hasattr(weights, 'grad') and weights.grad != None:\n",
    "            torch.addmm(\n",
    "                    weights.grad,\n",
    "                    neg_logprobs.T,\n",
    "                    hidden_states,\n",
    "                    out=weights.grad,\n",
    "                )\n",
    "        else:\n",
    "            weights.grad = neg_logprobs.T @ hidden_states\n",
    "        weights.grad_mul = False\n",
    "        \n",
    "        neg_logprobs = None\n",
    "\n",
    "        ctx.save_for_backward(grad_input, weights)\n",
    "        return result[mask].sum()\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, ctx, dneg_logprobs):\n",
    "        \"\"\"We know d(-log(p[i])/dlogit[k] = -id_mat[i,k] + p[k]\n",
    "        so we initialize the gradient as neg_logprobs, so we can just exponentiate\n",
    "        to get p[k], which is most of what we need...  neg_logprobs will be\n",
    "        modified in place to become the gradient we want\n",
    "        \"\"\"\n",
    "        # load saved tensors\n",
    "        neg_logprobs, weights = ctx.saved_tensors\n",
    "        # dneg_logprobs = dneg_logprobs / weights.mul\n",
    "        if weights.grad_mul is False:\n",
    "            weights.grad *= dneg_logprobs\n",
    "            weights.grad_mul = True\n",
    "        neg_logprobs *= dneg_logprobs\n",
    "        \n",
    "        return neg_logprobs, None, weights.grad\n",
    "\n",
    "\n",
    "class FusedCrossEntropyLMhead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_weight = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if original_weight is None:\n",
    "            self.LM_head_weight = nn.Parameter(torch.empty(hidden_size, vocab_size))\n",
    "        else:\n",
    "            self.LM_head_weight = original_weight\n",
    "        self.cross_entropy = _cross_entropy.apply\n",
    "\n",
    "    def forward(self, hidden_states, labels):\n",
    "        ignore_index = -100\n",
    "        loss = self.cross_entropy(hidden_states, labels, self.LM_head_weight)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "0528b159-4767-4133-b0a5-2fb8b61c83e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 263,  804,  414,  ..., -100, -100, -100], device='cuda:0')\n",
      "tensor(920.7166, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(895.4290, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(903.4427, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(911.4877, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(906.2062, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(910.1755, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(918.3792, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor(867.6902, device='cuda:0', grad_fn=<_cross_entropyBackward>)\n",
      "tensor([[ 2.3346e-03,  3.3188e-04,  3.4180e-03,  ..., -1.8921e-03,\n",
      "          1.0376e-03,  1.6708e-03],\n",
      "        [-1.1086e-05, -3.9041e-06,  5.3346e-06,  ...,  1.7062e-06,\n",
      "         -2.1219e-05, -5.0545e-05],\n",
      "        [-6.0320e-05,  2.0027e-05,  2.8759e-06,  ..., -3.9339e-05,\n",
      "          1.8001e-05, -3.5763e-05],\n",
      "        ...,\n",
      "        [-4.0054e-05,  4.2677e-05,  6.5565e-06,  ..., -7.8201e-05,\n",
      "          2.9802e-05,  2.7657e-05],\n",
      "        [ 1.7047e-05,  2.4438e-05,  3.7909e-05,  ..., -3.0994e-05,\n",
      "         -1.8358e-05, -2.7776e-05],\n",
      "        [-2.3556e-04,  9.0790e-04,  2.7084e-04,  ...,  1.0757e-03,\n",
      "          3.2806e-03,  1.3809e-03]], device='cuda:0', dtype=torch.bfloat16) tensor([[ 2.3346e-03,  3.2806e-04,  3.4332e-03,  ..., -1.8921e-03,\n",
      "          1.0376e-03,  1.6708e-03],\n",
      "        [-1.0908e-05, -3.9041e-06,  5.3644e-06,  ...,  1.7807e-06,\n",
      "         -2.1100e-05, -5.0783e-05],\n",
      "        [-6.0558e-05,  2.0027e-05,  2.7269e-06,  ..., -3.9577e-05,\n",
      "          1.8120e-05, -3.5763e-05],\n",
      "        ...,\n",
      "        [-4.0054e-05,  4.2677e-05,  6.6459e-06,  ..., -7.8201e-05,\n",
      "          2.9922e-05,  2.7657e-05],\n",
      "        [ 1.7285e-05,  2.4438e-05,  3.7670e-05,  ..., -3.0756e-05,\n",
      "         -1.8358e-05, -2.7895e-05],\n",
      "        [-2.3651e-04,  9.1553e-04,  2.7275e-04,  ...,  1.0681e-03,\n",
      "          3.2959e-03,  1.3885e-03]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[ 1.3530e-05, -1.0133e-05, -1.2815e-05,  ..., -2.8968e-05,\n",
      "          2.5749e-05,  4.5896e-06],\n",
      "        [-1.0192e-05, -1.9550e-05,  1.6093e-05,  ..., -1.5140e-05,\n",
      "         -2.4401e-07,  2.0027e-05],\n",
      "        [-2.9683e-05,  1.2279e-05, -2.6941e-05,  ...,  1.5736e-05,\n",
      "         -2.3365e-05,  2.2054e-06],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.bfloat16) tensor([[ 1.3590e-05, -1.0133e-05, -1.2815e-05,  ..., -2.8968e-05,\n",
      "          2.5749e-05,  4.5896e-06],\n",
      "        [-1.0133e-05, -1.9431e-05,  1.6093e-05,  ..., -1.5140e-05,\n",
      "         -2.4214e-07,  2.0027e-05],\n",
      "        [-2.9683e-05,  1.2279e-05, -2.6941e-05,  ...,  1.5736e-05,\n",
      "         -2.3484e-05,  2.2054e-06],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(0.0015, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(-2.5988e-05, device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor(7.1056, device='cuda:0', grad_fn=<DivBackward0>) tensor(7.1056, device='cuda:0', grad_fn=<NllLossBackward0>) tensor(4.7684e-07, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from triton.ops import cross_entropy\n",
    "\n",
    "M = 1024\n",
    "N = 1024\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.randn(M, N, dtype=dtype, device=device, requires_grad=True)\n",
    "x_test = x.detach().clone()\n",
    "x_test.requires_grad=True\n",
    "random_sequence = np.random.randint(low=0, high=N, size=(M,))\n",
    "idx = torch.tensor(random_sequence).cuda() \n",
    "idx[-5:] = -100\n",
    "print(idx)\n",
    "lm_head = nn.Linear(N, N, bias=False).cuda()\n",
    "lm_head.bfloat16()\n",
    "\n",
    "b_lm = lm_head.weight.detach().clone()\n",
    "\n",
    "th_y = torch.nn.CrossEntropyLoss(reduction=\"mean\")(lm_head(x[:-1, :]).float(), idx[1:])\n",
    "# tt_y = fuse(x_test[:-1, :], idx[1:])\n",
    "\n",
    "pretraining_tp = 8\n",
    "tmp = M // pretraining_tp\n",
    "hidden_states = x_test[:-1, :]\n",
    "labels = idx[1:]\n",
    "\n",
    "loss = None\n",
    "b_lm.mul = None\n",
    "for i in range(pretraining_tp):\n",
    "\n",
    "    Fused = FusedCrossEntropyLMhead(b_lm)\n",
    "\n",
    "    shift_hidden_states = hidden_states[i * tmp : (i+1)*tmp, :].contiguous()\n",
    "    shift_labels = labels[i * tmp : (i+1)*tmp ].contiguous()\n",
    "\n",
    "    loss_i = Fused(shift_hidden_states, shift_labels)\n",
    "\n",
    "    print(loss_i)\n",
    "\n",
    "    if loss is None:\n",
    "        loss = loss_i\n",
    "    else:\n",
    "        loss = loss + loss_i\n",
    "loss = loss / torch.sum(labels != -100)\n",
    "th_y.backward()\n",
    "loss.backward()\n",
    "\n",
    "print(lm_head.weight.grad, b_lm.grad)\n",
    "print(x.grad , x_test.grad)\n",
    "print(torch.sum(lm_head.weight.grad - b_lm.grad))\n",
    "print(torch.sum(x.grad - x_test.grad))\n",
    "\n",
    "print(loss, th_y, torch.sum(loss - th_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32721f05-4f2b-4b88-9fc8-d625b8fb54f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1657c9-ff3f-4b89-b732-e17f3504bc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
